{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN+Entropy_Min+Mixup.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I-9reXI-QVfk","colab_type":"code","outputId":"7ec144b7-181e-4f93-943f-49cb63b6ab7f","executionInfo":{"status":"error","timestamp":1575768929039,"user_tz":420,"elapsed":738244,"user":{"displayName":"Prashant Jadhav","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBxwSu-o99QLa6W50GUEL3XE5-8n-m8u3eyptECJA=s64","userId":"12969679806480930958"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# baseline model with weight decay on the cifar10 dataset\n","# CNN+Entropy Minimization+Mixup model on the cifar10 dataset\n","# Codes from below mentioned web site is used as a reference to build this model.\n","#https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n","#We have also refered to the code from below github link\n","#https://github.com/yu4u/mixup-generator\n","# This link contains the mixup generator code we used \n","#to augment data using mixup_generator and we have used it for our analysis\n","#Below is the licence copyright for mixup generator imported in this code\n","#MIT License\n","#Copyright (c) 2017 Yusuke Uchida\n","\n","#@author : Jayesh Kudase, Prashant Jadhav, Aditya Chavan\n","\n","import sys\n","from matplotlib import pyplot\n","from keras.datasets import cifar10\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","from keras.regularizers import l2\n","from mixup_generator import MixupGenerator\n","from keras.preprocessing.image import ImageDataGenerator\n","import keras.backend as k\n","import numpy as np\n","import sklearn.metrics as metrics\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","#labeled_size=25000\n","#unlabeled_size=25000\n","labeled_size=15000\n","unlabeled_size=35000\n","# labeled_data_size=5000\n","# unlabeled_data_size=45000\n","\n","# load train and test dataset\n","def load_dataset():\n","\t# load dataset\n","\t(train_X, train_Y), (test_X, test_Y1) = cifar10.load_data()\n","\t# one hot encode target values\n","\ttrain_Y = to_categorical(train_Y)\n","\ttest_Y = to_categorical(test_Y1)\n","\treturn train_X, train_Y, test_X, test_Y, test_Y1\n","\n","# scale pixels\n","def prep_pixels(train, test):\n","\t# convert from integers to floats\n","\ttraining_norm = train.astype('float32')\n","\ttesting_norm = test.astype('float32')\n","\t# normalize to range 0-1\n","\ttraining_norm = training_norm / 255.0\n","\ttesting_norm = testing_norm / 255.0\n","\t# return normalized images\n","\treturn training_norm, testing_norm\n","\n","\n","# define custom loss\n","def custom_loss(yTrue,yPred):\n","\tprint(yTrue[:1])\n","\tlab_loss=k.sum(yTrue[:labeled_size]*k.log(yPred[:labeled_size]))/(-labeled_size)\n","\tunlab_loss=k.sum(yPred[labeled_size:]*k.log(yPred[labeled_size:]))/(-unlabeled_size)\n","\treturn lab_loss+unlab_loss\n","\n","# define cnn modez\n","def define_model():\n","\tmodel = Sequential()\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n","\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n","\tmodel.add(Dense(10, activation='softmax'))\n","\t# compile model\n","\topt = SGD(lr=0.001, momentum=0.9)\n","\tmodel.compile(optimizer=opt, loss=custom_loss, metrics=['accuracy'])\n","\t\n","\treturn model\n","\n","# plot diagnostic learning curves\n","def summarize_diagnostics(history):\n","\t# plot loss\n","\tpyplot.subplot(211)\n","\tpyplot.title('Cross Entropy Loss')\n","\tpyplot.plot(history.history['loss'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n","\t# plot accuracy\n","\tpyplot.subplot(212)\n","\tpyplot.title('Classification Accuracy')\n","\tpyplot.plot(history.history['acc'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_acc'], color='orange', label='test')\n","\t# save plot to file\n","\tfilename = sys.argv[0].split('/')[-1]\n","\tpyplot.savefig(filename + '_plot.png')\n","\tpyplot.close()\n","\n","# run the test harness for evaluating a model\n","def run_test_harness():\n","\t# load dataset\n","\ttrain_X, train_Y, test_X, test_Y, test_Y1 = load_dataset()\n","\t# prepare pixel data\n","\ttrain_X, test_X = prep_pixels(train_X, test_X)\n","\t# define model\n","\tmodel = define_model()\n","\t# fit model\n","\t#history = model.fit(train_X, train_Y, epochs=100, batch_size=500, validation_data=(test_X, test_Y), verbose=1)\n","\tdatagen = ImageDataGenerator(\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    horizontal_flip=True)\n","\ttraining_generator = MixupGenerator(train_X, train_Y, batch_size=500, alpha=0.2, datagen=datagen)()\n","\thistory=model.fit_generator(generator=training_generator,\n","                        steps_per_epoch=500 ,\n","                        validation_data=(test_X, test_Y),\n","                        epochs=50, verbose=1)\n","\t#confusion matrix\n","\ty_pred_ohe = model.predict(test_X) \n","\ty_pred_labels = np.argmax(y_pred_ohe, axis=1) \n","\tconfusion_matrix = metrics.confusion_matrix(y_true=test_Y1, y_pred=y_pred_labels)\n","\tfigure = pyplot.figure(figsize=(8, 8))\n","\tsns.heatmap(confusion_matrix, annot=False,cmap=pyplot.cm.Blues)\n","\tpyplot.tight_layout()\n","\tpyplot.ylabel('True label')\n","\tpyplot.xlabel('Predicted label')\n","\tpyplot.imshow(confusion_matrix)\n","\t# evaluate model\n","\t_, acc = model.evaluate(test_X, test_Y, verbose=0)\n","\tprint('> %.3f' % (acc * 100.0))\n","\t# learning curves\n","\t#summarize_diagnostics(history)\n","\n","# entry point, run the test harness\n","run_test_harness()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 11s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Tensor(\"loss/dense_2_loss/strided_slice:0\", shape=(?, ?), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/50\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","500/500 [==============================] - 157s 314ms/step - loss: 1.2157 - acc: 0.1844 - val_loss: 1.1997 - val_acc: 0.2511\n","Epoch 2/50\n","500/500 [==============================] - 150s 300ms/step - loss: 1.1883 - acc: 0.2558 - val_loss: 1.1743 - val_acc: 0.2970\n","Epoch 3/50\n","500/500 [==============================] - 150s 300ms/step - loss: 1.1641 - acc: 0.2853 - val_loss: 1.1502 - val_acc: 0.3178\n","Epoch 4/50\n","500/500 [==============================] - 152s 304ms/step - loss: 1.1411 - acc: 0.3036 - val_loss: 1.1272 - val_acc: 0.3393\n","Epoch 5/50\n","500/500 [==============================] - 152s 304ms/step - loss: 1.1190 - acc: 0.3167 - val_loss: 1.1052 - val_acc: 0.3531\n","Epoch 6/50\n","500/500 [==============================] - 152s 305ms/step - loss: 1.0975 - acc: 0.3266 - val_loss: 1.0837 - val_acc: 0.3629\n","Epoch 7/50\n","500/500 [==============================] - 152s 303ms/step - loss: 1.0766 - acc: 0.3360 - val_loss: 1.0630 - val_acc: 0.3699\n","Epoch 8/50\n","500/500 [==============================] - 152s 303ms/step - loss: 1.0562 - acc: 0.3435 - val_loss: 1.0426 - val_acc: 0.3774\n","Epoch 9/50\n","500/500 [==============================] - 151s 303ms/step - loss: 1.0363 - acc: 0.3493 - val_loss: 1.0227 - val_acc: 0.3824\n","Epoch 10/50\n","500/500 [==============================] - 152s 304ms/step - loss: 1.0168 - acc: 0.3548 - val_loss: 1.0033 - val_acc: 0.3866\n","Epoch 11/50\n","500/500 [==============================] - 151s 303ms/step - loss: 0.9978 - acc: 0.3577 - val_loss: 0.9844 - val_acc: 0.3934\n","Epoch 12/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.9790 - acc: 0.3636 - val_loss: 0.9658 - val_acc: 0.3962\n","Epoch 13/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.9609 - acc: 0.3660 - val_loss: 0.9477 - val_acc: 0.4019\n","Epoch 14/50\n","500/500 [==============================] - 151s 302ms/step - loss: 0.9430 - acc: 0.3705 - val_loss: 0.9298 - val_acc: 0.4061\n","Epoch 15/50\n","500/500 [==============================] - 151s 301ms/step - loss: 0.9254 - acc: 0.3749 - val_loss: 0.9124 - val_acc: 0.4083\n","Epoch 16/50\n","500/500 [==============================] - 151s 303ms/step - loss: 0.9083 - acc: 0.3771 - val_loss: 0.8953 - val_acc: 0.4154\n","Epoch 17/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.8915 - acc: 0.3799 - val_loss: 0.8786 - val_acc: 0.4191\n","Epoch 18/50\n","500/500 [==============================] - 152s 304ms/step - loss: 0.8750 - acc: 0.3832 - val_loss: 0.8623 - val_acc: 0.4144\n","Epoch 19/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.8588 - acc: 0.3857 - val_loss: 0.8461 - val_acc: 0.4243\n","Epoch 20/50\n","500/500 [==============================] - 152s 305ms/step - loss: 0.8430 - acc: 0.3898 - val_loss: 0.8304 - val_acc: 0.4267\n","Epoch 21/50\n","500/500 [==============================] - 152s 304ms/step - loss: 0.8275 - acc: 0.3909 - val_loss: 0.8151 - val_acc: 0.4254\n","Epoch 22/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.8124 - acc: 0.3937 - val_loss: 0.7998 - val_acc: 0.4349\n","Epoch 23/50\n","500/500 [==============================] - 151s 303ms/step - loss: 0.7975 - acc: 0.3955 - val_loss: 0.7851 - val_acc: 0.4342\n","Epoch 24/50\n","500/500 [==============================] - 151s 302ms/step - loss: 0.7829 - acc: 0.3975 - val_loss: 0.7707 - val_acc: 0.4386\n","Epoch 25/50\n","500/500 [==============================] - 152s 305ms/step - loss: 0.7688 - acc: 0.3982 - val_loss: 0.7565 - val_acc: 0.4419\n","Epoch 26/50\n","500/500 [==============================] - 151s 302ms/step - loss: 0.7547 - acc: 0.4018 - val_loss: 0.7428 - val_acc: 0.4370\n","Epoch 27/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.7411 - acc: 0.4028 - val_loss: 0.7291 - val_acc: 0.4381\n","Epoch 28/50\n","500/500 [==============================] - 149s 298ms/step - loss: 0.7276 - acc: 0.4053 - val_loss: 0.7162 - val_acc: 0.4402\n","Epoch 29/50\n","500/500 [==============================] - 151s 302ms/step - loss: 0.7146 - acc: 0.4062 - val_loss: 0.7027 - val_acc: 0.4474\n","Epoch 30/50\n","500/500 [==============================] - 151s 301ms/step - loss: 0.7017 - acc: 0.4076 - val_loss: 0.6901 - val_acc: 0.4426\n","Epoch 31/50\n","500/500 [==============================] - 152s 303ms/step - loss: 0.6890 - acc: 0.4088 - val_loss: 0.6773 - val_acc: 0.4466\n","Epoch 32/50\n","500/500 [==============================] - 152s 305ms/step - loss: 0.6766 - acc: 0.4115 - val_loss: 0.6654 - val_acc: 0.4438\n","Epoch 33/50\n","500/500 [==============================] - 152s 304ms/step - loss: 0.6646 - acc: 0.4115 - val_loss: 0.6534 - val_acc: 0.4480\n","Epoch 34/50\n","500/500 [==============================] - 155s 309ms/step - loss: 0.6527 - acc: 0.4138 - val_loss: 0.6414 - val_acc: 0.4501\n","Epoch 35/50\n","500/500 [==============================] - 156s 313ms/step - loss: 0.6411 - acc: 0.4145 - val_loss: 0.6297 - val_acc: 0.4508\n","Epoch 36/50\n","500/500 [==============================] - 153s 305ms/step - loss: 0.6297 - acc: 0.4160 - val_loss: 0.6186 - val_acc: 0.4521\n","Epoch 37/50\n","500/500 [==============================] - 154s 307ms/step - loss: 0.6185 - acc: 0.4153 - val_loss: 0.6074 - val_acc: 0.4528\n","Epoch 38/50\n","500/500 [==============================] - 152s 305ms/step - loss: 0.6076 - acc: 0.4163 - val_loss: 0.5973 - val_acc: 0.4448\n","Epoch 39/50\n","500/500 [==============================] - 154s 307ms/step - loss: 0.5969 - acc: 0.4189 - val_loss: 0.5863 - val_acc: 0.4559\n","Epoch 40/50\n","500/500 [==============================] - 155s 309ms/step - loss: 0.5864 - acc: 0.4187 - val_loss: 0.5757 - val_acc: 0.4563\n","Epoch 41/50\n","500/500 [==============================] - 159s 319ms/step - loss: 0.5760 - acc: 0.4210 - val_loss: 0.5655 - val_acc: 0.4550\n","Epoch 42/50\n","500/500 [==============================] - 154s 307ms/step - loss: 0.5660 - acc: 0.4212 - val_loss: 0.5552 - val_acc: 0.4622\n","Epoch 43/50\n","500/500 [==============================] - 155s 310ms/step - loss: 0.5561 - acc: 0.4204 - val_loss: 0.5454 - val_acc: 0.4616\n","Epoch 44/50\n","500/500 [==============================] - 154s 308ms/step - loss: 0.5463 - acc: 0.4240 - val_loss: 0.5360 - val_acc: 0.4619\n","Epoch 45/50\n","500/500 [==============================] - 154s 307ms/step - loss: 0.5368 - acc: 0.4241 - val_loss: 0.5264 - val_acc: 0.4668\n","Epoch 46/50\n","123/500 [======>.......................] - ETA: 1:54 - loss: 0.5310 - acc: 0.4270"],"name":"stdout"},{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-1-bb4ccd2df8ef>\", line 129, in <module>\n","    run_test_harness()\n","  File \"<ipython-input-1-bb4ccd2df8ef>\", line 111, in run_test_harness\n","    epochs=50, verbose=1)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1658, in fit_generator\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 181, in fit_generator\n","    generator_output = next(output_generator)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 702, in get\n","    inputs = future.get(timeout=30)\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 638, in get\n","    self.wait(timeout)\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 635, in wait\n","    self._event.wait(timeout)\n","  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n","    signaled = self._cond.wait(timeout)\n","  File \"/usr/lib/python3.6/threading.py\", line 299, in wait\n","    gotit = waiter.acquire(True, timeout)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n","    if ismodule(module) and hasattr(module, '__file__'):\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n","    module = self._load()\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n","    module = _importlib.import_module(self.__name__)\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/__init__.py\", line 72, in <module>\n","    from tensorflow.contrib import periodic_resample\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/periodic_resample/__init__.py\", line 22, in <module>\n","    from tensorflow.contrib.periodic_resample.python.ops.periodic_resample_op import periodic_resample\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/periodic_resample/__init__.py\", line 22, in <module>\n","    from tensorflow.contrib.periodic_resample.python.ops.periodic_resample_op import periodic_resample\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/periodic_resample/python/ops/periodic_resample_op.py\", line 32, in <module>\n","    resource_loader.get_path_to_datafile('_periodic_resample_op.so'))\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/util/loader.py\", line 56, in load_op_library\n","    ret = load_library.load_op_library(path)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n","    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n","KeyboardInterrupt\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]}]}